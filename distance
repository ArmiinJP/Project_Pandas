با این صورت مسئله رو به رو هستیم:
معیار مناسب برای پیدا کردن تفاوت بین دو توزیع احتمالاتی چیست؟
سوال بالا را میتوان به نحوی بهتر نیز مطرح کرد، و آن به این صورت است که فاصله آماری بین دو توزیع احتمال را چگونه می‌توان محاسبه کرد؟
در ماشین لرنینگ به پیدا کردن این فاصله، اندازه‌گیری شباهت (similarity) یا واگرایی (divergence) بین دو توزیع احتمالاتی می‌گویند.

سوالی که مطرح هست فایده‌ی پیدا کردن این شباهت چیست؟
در ماشین لرنینگ، با توزیع های احتمالاتی هم از نوع پیوسته و هم گسسته، در داده‌ی ورودی، داده های خروجی از مدل، هنگام محاسبه‌ی خطا بین داده های واقعی خروجی، و داده های پیش‌بینی خروجی و... روبه رو هستیم. 
اندازه گیری توزیع احتمال همه ویژگی های ورودی و خروجی به شناسایی انحراف داده ها کمک می کند.
هنگام آموزش مدل ها، هدف ما به حداقل رساندن خطا می‌باشد. یکی از روشها دانستن اطلاعات از دسته رفته می‌باشد که با محاسبه‌ی  واگرایی قابل دستیابی است.

روش های متداول برای اندازه‌گیری شباهت بین دو توزیع موارد زیر هستند:
روش KLD یا Kullback Leiber Divergence 
روش JSD یا Jensen-Shannon Divergence
روش Kolmogorov–Smirnov
که در ادامه به توضیح هر یک خواهم پرداخت.


روش Kullback–Leibler Divergence(KL Divergence)
هدف این روش اندازه گیری شباهت بین دو توزیع احتمالی گسسته  P و Q می‌باشد، که در آن P نشان دهنده توزیع داده های واقعی، و Q نشان دهنده توزیع داده های پیش بینی شده است.

این روش همچنین آنتروپی نسبی P نسبت به Q نیز نامیده می‌شود در واقع اطلاعات از دست رفته هنگام حرکت از P به Q را تبدیل به داده های کمی می کند. در عمل می‌توان گفت این روش یک متریک واقعی برای پیدا کردن فاصله‌ی بین P و Q نیست بلکه در عمل اندازه گیری واگرایی بین این دو توزیع را انجام می‌دهد.

روش محاسبه:
عکس
https://medium.com/geekculture/techniques-to-measure-probability-distribution-similarity-9145678d68a6

خروجی این روش یک مقدار بین  [0, +∞) می‌باشد.
که در آن مقادیر نزدیک به صفر نشان از شباهت میان دو توزیع برای fact های مختلف دارد.
و مقادیر مثبت به این معناست که توزیع لیبل ها واگر می‌باشد، و هر چه مقدار بیشتر باشد، میزان واگرایی نیز بیشتر می‌باشد.

ویژگی های KL Divergence
یکی از ویژگی های این روش نامتقارن بودن آن است یعنی در عمل فاصله‌ای که بین P , Q محاسبه‌ می‌شود با فاصله‌ی Q, P متفاوت می‌باشد:
Dₖₗ(P|Q) ≠ Dₖₗ(Q|P)
همانطور که در بالا اشاره کردم، این روش محاسبه‌ی یک متریک واقعی برای فاصله بین P و Q نیست، حتی اگر فاصله بین دو توزیع داده را اندازه گیری کند.

این روش نابرابری مثلثی را برآورده نمی کند. یعنی:
c < a + b 
ممکن است برقرار نباشد.

این روش نامحدود است زیرا مقادیر نتایج متریک KL از [0، +∞) متغیر است.


روش JSD یا Jensen-Shannon Divergence
روشی برای اندازه گیری شباهت بین دو توزیع احتمال P و Q. همچنین به عنوان شعاع اطلاعات یا واگرایی کل به میانگین شناخته می شود.

این روش نسخه متقارن شده‌ی KL Divergence می‌باشد. 
مقدار خروجی این روش بر خلاف روش قبلی محدود شده می‌باشد و بین [0, 1] برای لگاریتم در پایه‌ی 2 و برای لگاریتم در پایه‌ی 2 در محدوده‌ی [0, ln(2)] می‌باشد.
مقادیر نزدیک به صفر شباهت بین توزیع ها را نشان می دهد.
مقادیر مثبت نشان دهنده واگرایی بین توزیعها است. هر چه مقدار این عدد بزرگتر باشد واگرایی نیز بیشتر خواهد بود.

همچنین برخلاف روش قبلی که یک متریک واقعی برای فاصله‌ی بین دو توزیع نبود، در اینجا فاصله‌ی بین دو توزیع قابل محاسبه و برابر با جذر JSD می‌باشد، یعنی فاصله‌ی Jensen-Shannon برابر با جذر JSD می‌باشد.

ویژگی‌ها:
بر پایه‌ی KL divergence بوده و متقارن هست، برخلاف آن رابطه‌ی زیر در این روش برقرار هست.
JDS(P|Q)=JSD(Q|P)
خروجی محدود بوده و بین 0 , 1 برای لگاریتم بر پایه‌ی 2 باند شده است، همین امر منجر به آن شده که نسخه‌ی smoothedو normalized شده‌ی  KL Divergence
باشد.

روش Kolmogorov-Smirnov (KS) Test 




در نهایت divergence KL، divergence JS و آزمون KS تکنیک هایی برای اندازه گیری شباهت یا تفاوت آماری بین توزیع ها هستند.
روش KL divergence یک معیار محاسبه‌ی divergence نامحدود است، نه یک متریک فاصله. نامتقارن است و نابرابری مثلثی را برآورده نمی کند.
روش JS divergence نسخه‌ی محدود شده و متقارن KL divergence می‌باشد. و در نهایت یک اندازه گیری ناپارامتریک پیوسته برای توزیع داده های یک بعدی است.
\

مقایسه‌ی میان KLD و JSD : در اینجا باید گفت KLD یک تابع نامتقارن است. از آنجایی که ممکن است محاسبات مسافت زیادی مورد نیاز بوده باشد، این یک ریسک خواهد بود.
در مقابل JSD یک تابع متقارن است و جذر JSD فاصله Jensen-Shannon را نشان می دهد. معیاری که می توانیم از آن برای یافتن شباهت بین دو توزیع احتمال استفاده کنیم. 0 نشان می دهد که دو توزیع یکسان هستند و 1 نشان می دهد که هیچ کجا مشابه نیستند.


