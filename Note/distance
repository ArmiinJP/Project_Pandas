

روش های متداول برای اندازه‌گیری شباهت بین دو توزیع موارد زیر هستند:
روش KLD یا Kullback Leiber Divergence 
روش JSD یا Jensen-Shannon Divergence
روش Kolmogorov–Smirnov
روش Bhattacharyya Distance
که در ادامه به توضیح هر یک خواهم پرداخت.

روش Kullback–Leibler Divergence(KL Divergence)
هدف این روش اندازه گیری شباهت بین دو توزیع احتمالی گسسته  P و Q می‌باشد، که در آن P نشان دهنده توزیع داده های واقعی، و Q نشان دهنده توزیع داده های پیش بینی شده است.

این روش همچنین آنتروپی نسبی P نسبت به Q نیز نامیده می‌شود در واقع اطلاعات از دست رفته هنگام حرکت از P به Q را تبدیل به داده های کمی می کند. در عمل می‌توان گفت این روش یک متریک واقعی برای پیدا کردن فاصله‌ی بین P و Q نیست بلکه در عمل اندازه گیری واگرایی بین این دو توزیع را انجام می‌دهد.
فرمول محاسبه‌ی این روش در زیر آورده شده است:
عکس

خروجی این روش یک مقدار بین  صفر تا مثبت بی‌نهایت می‌باشد.
که در آن مقادیر نزدیک به صفر نشان از شباهت میان دو توزیع دارد.
و مقادیر مثبت به این معناست که توزیع لیبل ها واگر می‌باشد، و هر چه مقدار بیشتر باشد، میزان واگرایی نیز بیشتر می‌باشد.

ویژگی های KL Divergence
یکی از ویژگی های این روش نامتقارن بودن آن است یعنی در عمل فاصله‌ای که بین P , Q محاسبه‌ می‌شود با فاصله‌ی Q, P متفاوت می‌باشد:
Dₖₗ(P|Q) ≠ Dₖₗ(Q|P)
همانطور که در بالا اشاره کردم، این روش محاسبه‌ی یک متریک واقعی برای فاصله بین P و Q نیست، حتی اگر فاصله بین دو توزیع داده را اندازه گیری کند.

این روش نابرابری مثلثی را برآورده نمی کند. یعنی:

ممکن است برقرار نباشد.

این روش نامحدود یا unbounded است زیرا مقادیر نتایج متریک KL از [0، +∞) متغیر است.

نمونه‌ پیاده‌سازی
کد مربوط به این روش در زیر آورده شده است:
خروجی کد بالا در تصاویر زیر آورده شده است، همانطور که مشخص هست، مقدار KL برای KL_div(p, q) با KL_div(q, p) متفاوت می‌باشد و این نشان از نامتقارن بودن این روش که در بالا به آن اشاره شد دارد.



روش JSD یا Jensen-Shannon Divergence
روشی برای اندازه گیری شباهت بین دو توزیع احتمال P و Q. همچنین به عنوان شعاع اطلاعات یا واگرایی کل به میانگین شناخته می شود.

این روش نسخه متقارن شده‌ی KL Divergence می‌باشد. 
مقدار خروجی این روش بر خلاف روش قبلی محدود شده می‌باشد و بین [0, 1] برای لگاریتم در پایه‌ی 2 و برای لگاریتم در پایه‌ی 2 در محدوده‌ی [0, ln(2)] می‌باشد.
مقادیر نزدیک به صفر شباهت بین توزیع ها را نشان می دهد.
مقادیر مثبت نشان دهنده واگرایی بین توزیعها است. هر چه مقدار این عدد بزرگتر باشد واگرایی نیز بیشتر خواهد بود.

همچنین برخلاف روش قبلی که یک متریک واقعی برای فاصله‌ی بین دو توزیع نبود، در اینجا فاصله‌ی بین دو توزیع قابل محاسبه و برابر با جذر JSD می‌باشد، یعنی فاصله‌ی Jensen-Shannon برابر با جذر JSD می‌باشد.

ویژگی‌ها:
بر پایه‌ی KL divergence بوده و متقارن هست، برخلاف آن رابطه‌ی زیر در این روش برقرار هست.
JDS(P|Q)=JSD(Q|P)
خروجی محدود بوده و بین 0 , 1 برای لگاریتم بر پایه‌ی 2 باند شده است، همین امر منجر به آن شده که نسخه‌ی smoothed و normalized شده‌ی  KL Divergence باشد.

روش Kolmogorov-Smirnov (KS) Test 




در نهایت divergence KL، divergence JS و آزمون KS تکنیک هایی برای اندازه گیری شباهت یا تفاوت آماری بین توزیع ها هستند.
روش KL divergence یک معیار محاسبه‌ی divergence نامحدود است، نه یک متریک فاصله. نامتقارن است و نابرابری مثلثی را برآورده نمی کند.
روش JS divergence نسخه‌ی محدود شده و متقارن KL divergence می‌باشد. و در نهایت یک اندازه گیری ناپارامتریک پیوسته برای توزیع داده های یک بعدی است.
\

مقایسه‌ی میان KLD و JSD : در اینجا باید گفت KLD یک تابع نامتقارن است. از آنجایی که ممکن است محاسبات مسافت زیادی مورد نیاز بوده باشد، این یک ریسک خواهد بود.
در مقابل JSD یک تابع متقارن است و جذر JSD فاصله Jensen-Shannon را نشان می دهد. معیاری که می توانیم از آن برای یافتن شباهت بین دو توزیع احتمال استفاده کنیم. 0 نشان می دهد که دو توزیع یکسان هستند و 1 نشان می دهد که هیچ کجا مشابه نیستند.


