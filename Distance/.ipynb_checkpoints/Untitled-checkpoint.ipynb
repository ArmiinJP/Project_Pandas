{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e95f834-b6f4-4c10-818d-3b6322806f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65682770-2499-4aa9-9f82-f7f9db999e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = pd.read_csv('dist_data.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cf057cf-652a-44d1-9a70-ef1cad0f05f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dist.iloc[:, :-1].values\n",
    "y = dist.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1640004-950d-4e65-bca3-328047c43c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e40cd4c7-12ae-41df-a54d-8a18539ec879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sample(sample):\n",
    "    sample_list = sample[0].replace(', [', '; [').split(';')\n",
    "    sample_x = sample_list[0]\n",
    "    sample_Px = sample_list[1]\n",
    "\n",
    "    sample_x_list = []\n",
    "    sample_Px_list = []\n",
    "    \n",
    "    for x in sample_x.split(','):\n",
    "        sample_x_list.append(x.replace(' ', '').replace('[', '').replace(']', ''))\n",
    "    for Px in sample_Px.split(','):\n",
    "        sample_Px_list.append(Px.replace(' ', '').replace('[', '').replace(']', ''))\n",
    "\n",
    "    dict_sample = {}\n",
    "    for i in range(len(sample_x_list)):\n",
    "        dict_sample[sample_x_list[i]] = sample_Px_list[i]\n",
    "    return dict_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8494591-0ad1-4c0b-a29b-f8b7ea40f8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_keys_dataset1_exist_in_dataset2(dataset1, dataset2, threshold):\n",
    "    count = 0\n",
    "    total = len(dataset1.keys())\n",
    "    for key in dataset1.keys():\n",
    "        if dataset2.get(key) is None:\n",
    "            pass\n",
    "        else:\n",
    "            count += 1\n",
    "\n",
    "    if (count/total) * 100 >= float(threshold):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cfc4e7b8-b860-4dc2-a692-697f121de47a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calculate_distance(distance_func, min_precent_similarity):\n",
    "    count = 0\n",
    "    result = []\n",
    "    for i, sample_x_test in enumerate(x_test): \n",
    "        result_zikma = []\n",
    "    \n",
    "        for j, sample_x_train in enumerate(x_train):\n",
    "            if all_keys_dataset1_exist_in_dataset2(clean_sample(sample_x_test), clean_sample(sample_x_train), min_precent_similarity):\n",
    "    \n",
    "                Px_list = []\n",
    "                for xd1, Pxd1 in clean_sample(sample_x_test).items():\n",
    "                    if xd1 == \"\" or Pxd1 == \"\":\n",
    "                        continue\n",
    "                    Pxd2 = clean_sample(sample_x_train).get(xd1)\n",
    "                    if Pxd2 is None:\n",
    "                        continue\n",
    "                    Px_list.append([Pxd1, Pxd2])\n",
    "            \n",
    "                sum = distance_func(Px_list)\n",
    "                result_zikma.append([y_train[j], sum])\n",
    "                \n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        result.append([sample_x_test ,result_zikma])\n",
    "        \n",
    "        count += 1\n",
    "        if count > 100:\n",
    "            break\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "339bf2ef-4886-4030-9673-7b14be0d4594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def JS_Div(p, q):\n",
    "    p = np.asarray(p)\n",
    "    q = np.asarray(q)\n",
    "    # normalize\n",
    "    p /= p.sum()\n",
    "    q /= q.sum()\n",
    "    m = (p + q) / 2\n",
    "    return (KL_div(p, m) + KL_div(q, m)) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9a2a6bdc-c54b-4de6-95d8-68fef469836f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl(Px_list):\n",
    "    sum = 0 \n",
    "    for i in Px_list:\n",
    "        Pxd1 = i[0]\n",
    "        Pxd2 = i[1]\n",
    "        sum += float(Pxd1) * math.log10(float(Pxd1) / float(Pxd2))\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a8fb47-c84f-456d-9235-76d0d6d6404d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in range(1, 100): # one or zero\n",
    "    result.append(calculate_distance(kl, i))\n",
    "    if i == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "111fcc81-9a00-4668-a558-51b39043b13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result = calculate_distance(kl, 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3d5467de-3f55-4faa-a7cd-e9c972a33e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using max\n",
    "y_pred = []\n",
    "for i in range(len(x_test) - 2010):\n",
    "    t = pd.DataFrame(result[0][i][1], columns=['Label', 'Kl Result'])\n",
    "    bests = t.value_counts()\n",
    "    if bests.size == 0:\n",
    "        y_pred.append(0)\n",
    "    else:\n",
    "        y_pred.append(bests.head(1).index[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f59cbd91-8aa0-49a9-ae97-cb1bf5720fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 6  0  0  1  0  0  0]\n",
      " [ 0 21  0  5  0  0  0]\n",
      " [ 0  0  1  0  0  0  0]\n",
      " [ 0  2  0 42  0  0  0]\n",
      " [ 0  1  1 12  3  0  0]\n",
      " [ 0  0  0  2  0  0  0]\n",
      " [ 0  4  0  0  0  0  0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       27311       1.00      0.86      0.92         7\n",
      "       27321       0.75      0.81      0.78        26\n",
      "       27341       0.50      1.00      0.67         1\n",
      "       27523       0.68      0.95      0.79        44\n",
      "       27525       1.00      0.18      0.30        17\n",
      "       27652       0.00      0.00      0.00         2\n",
      "       28090       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.72       101\n",
      "   macro avg       0.56      0.54      0.49       101\n",
      "weighted avg       0.73      0.72      0.67       101\n",
      "\n",
      "Accuracy: 0.7227722772277227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arminjp/Documents/Project/pandas/.venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/arminjp/Documents/Project/pandas/.venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/arminjp/Documents/Project/pandas/.venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "res = confusion_matrix(y_test[:101], y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(res)\n",
    "res1 = classification_report(y_test[:101], y_pred)\n",
    "print(\"Classification Report:\",)\n",
    "print (res1)\n",
    "res2 = accuracy_score(y_test[:101],y_pred)\n",
    "print(\"Accuracy:\",res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8276059c-a3a7-429d-a146-78cf62663619",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
